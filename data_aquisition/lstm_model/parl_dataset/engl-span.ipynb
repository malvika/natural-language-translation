{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data files and save them on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data(old_file_address, new_file_address):\n",
    "    \n",
    "    import string\n",
    "    from unicodedata import normalize\n",
    "    import re\n",
    "    \n",
    "    remove_punct_map = dict.fromkeys(map(ord, string.punctuation)) # thank you Reed!\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    \n",
    "    with open(old_file_address, 'r', encoding='utf-8') as old_file, open(new_file_address, 'w', encoding='utf-8') as clean_file:\n",
    "        for line in old_file:\n",
    "            \n",
    "            # normalize all Unicode characters to ASCII (maybe not nesessary)\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            \n",
    "            # remove non-printable characters\n",
    "            line = re_print.sub('', line)\n",
    "            \n",
    "            # normalize to lowercase, remove punctuation and write to a file\n",
    "            clean_file.write(line.lower().translate(remove_punct_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data('europarl-v7esp.txt', 'clean_spanish.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data('europarl-v7eng.txt', 'clean_english.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Observation with MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data contains millions of rows, so I used MRJob to handle it and test locally\n",
    "# Use boilerplate to count the number of words and lines\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import string\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation)) #thank you Reed\n",
    "\n",
    "class TextOverview(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        line = line.translate(remove_punct_map)\n",
    "        line = line.lower()\n",
    "        \n",
    "        yield \"total_words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TextOverview.run()\n",
    "    \n",
    "#python engl-span.py clean_english.txt\n",
    "#python engl-span.py clean_spanish.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count a number of unique words in each dataset:\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import string\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "class TextOverview(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        line = line.translate(remove_punct_map)\n",
    "        line = line.lower()\n",
    "        for word in line.split():\n",
    "            yield 'words', word\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, len(set(values))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TextOverview.run()\n",
    "\n",
    "#python engl-span.py clean_english.txt\n",
    "#python engl-span.py clean_spanish.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Output for english file:***\n",
    "- \"total_words\"   48 978 039\n",
    "- \"lines\" 1 965 734\n",
    "- \"unique_words\" 133 052"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Output for spanish file:***\n",
    "- \"total_words\"   51 505 465\n",
    "- \"lines\" 1 965 734\n",
    "- \"unique_words\" 192 038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined clean txt file on disk just in case\n",
    "with open('clean_english.txt', 'r') as eng, open('clean_spanish.txt', 'r') as span, open('combined.txt','w') as comb:\n",
    "    \n",
    "        for eline, sline in zip(eng, span):\n",
    "            eline = eline.rstrip()\n",
    "            sline = sline.rstrip()\n",
    "            \n",
    "            comb.write(f'{eline}\\t{sline}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(doc):\n",
    "    file = open(doc, mode='r', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    lines = text.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both files in one\n",
    "pairs = to_pairs('combined.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(eng_file_address, esp_file_address):\n",
    "    \n",
    "    from numpy import array\n",
    "    \n",
    "    \n",
    "    all_pairs = []\n",
    "    with open(eng_file_address, 'r') as eng, open(esp_file_address, 'r') as span:\n",
    "    \n",
    "        for eline, sline in zip(eng, span):\n",
    "            eline = eline.rstrip()\n",
    "            sline = sline.rstrip()\n",
    "\n",
    "            all_pairs.append([eline, sline])\n",
    "        \n",
    "    return array(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_array = create_array('clean_english.txt', 'clean_spanish.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. resumption of the session : reanudacion del periodo de sesiones\n",
      "1. i declare resumed the session of the european parliament adjourned on friday 17 december 1999 and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period : declaro reanudado el periodo de sesiones del parlamento europeo interrumpido el viernes 17 de diciembre pasado y reitero a sus senorias mi deseo de que hayan tenido unas buenas vacaciones\n",
      "2. although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful : como todos han podido comprobar el gran efecto del ano 2000 no se ha producido en cambio los ciudadanos de varios de nuestros paises han sido victimas de catastrofes naturales verdaderamente terribles\n",
      "3. you have requested a debate on this subject in the course of the next few days during this partsession : sus senorias han solicitado un debate sobre el tema para los proximos dias en el curso de este periodo de sesiones\n",
      "4. in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union : a la espera de que se produzca de acuerdo con muchos colegas que me lo han pedido pido que hagamos un minuto de silencio en memoria de todas las victimas de las tormentas en los distintos paises de la union europea afectados\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print(f'{x}. {pairs_array[x,0]} : {pairs_array[x,1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to plk file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#don't need this step\n",
    "def save_data(pairs, filename):\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    joblib.dump(pairs, filename) \n",
    "    \n",
    "    print(f'Saved: {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_data(pairs_array, 'english-spanish.pkl') #64Gb wow!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test data and save to the file\n",
    "def split_data(data, number_of_rows):\n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    from numpy.random import rand\n",
    "    from numpy.random import shuffle\n",
    "    \n",
    "    shuffle(data)\n",
    "    dataset = data[:number_of_rows, :]\n",
    "    shuffle(dataset)\n",
    "    \n",
    "    unique = {}\n",
    "    words = []\n",
    "    for pair in dataset:\n",
    "        for sentence in pair:\n",
    "            for word in sentence.split(' '):\n",
    "                words.append(word)\n",
    "                unique[word] = 1\n",
    "                    \n",
    "    print(f'a number of unique words - {len(unique)}')\n",
    "    print(f'a total number of words - {len(words)}')\n",
    "    \n",
    "    train, test = dataset[:(int(number_of_rows*0.8))], dataset[(int(number_of_rows*0.8)):]\n",
    "    \n",
    "    joblib.dump(dataset, 'dataset.pkl')\n",
    "    joblib.dump(train, 'train.pkl')\n",
    "    joblib.dump(test, 'test.pkl') \n",
    "    \n",
    "    return('done')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a number of unique words - 20477\n",
      "a total number of words - 257345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data(pairs_array, 5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
